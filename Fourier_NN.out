## SLURM PROLOG ###############################################################
##    Job ID : 1870474
##  Job Name : Fourier_NN
##  Nodelist : gpu2004
##      CPUs : 
##  Mem/Node : 153600 MB
## Directory : /gpfs/home/jsolt/FourierNN
##   Started : Thu Jul 29 14:58:49 EDT 2021
###############################################################################
stderr: /gpfs/home/jsolt/FourierNN/torchenv/lib/python3.7/site-packages/torch/distributed/launch.py:164: DeprecationWarning: The 'warn' method is deprecated, use 'warning' instead
stderr:   "The module torch.distributed.launch is deprecated "
stderr: The module torch.distributed.launch is deprecated and going to be removed in future.Migrate to torch.distributed.run
stderr: INFO:torch.distributed.launcher.api:Starting elastic_operator with launch configs:
stderr:   entrypoint       : /gpfs/home/jsolt/FourierNN/torchenv/lib/python3.7/site-packages/accelerate/test_utils/test_script.py
stderr:   min_nodes        : 1
stderr:   max_nodes        : 1
stderr:   nproc_per_node   : 4
stderr:   run_id           : none
stderr:   rdzv_backend     : static
stderr:   rdzv_endpoint    : 127.0.0.1:29500
stderr:   rdzv_configs     : {'rank': 0, 'timeout': 900}
stderr:   max_restarts     : 3
stderr:   monitor_interval : 5
stderr:   log_dir          : None
stderr:   metrics_cfg      : {}
stderr: 
stderr: INFO:torch.distributed.elastic.agent.server.local_elastic_agent:log directory set to: /tmp/torchelastic_hqci49x8/none_273htobb
stderr: INFO:torch.distributed.elastic.agent.server.api:[default] starting workers for entrypoint: python3
stderr: INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group
stderr: /gpfs/home/jsolt/FourierNN/torchenv/lib/python3.7/site-packages/torch/distributed/elastic/utils/store.py:53: FutureWarning: This is an experimental API and will be changed in future.
stderr:   "This is an experimental API and will be changed in future.", FutureWarning
stderr: INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:
stderr:   restart_count=0
stderr:   master_addr=127.0.0.1
stderr:   master_port=29500
stderr:   group_rank=0
stderr:   group_world_size=1
stderr:   local_ranks=[0, 1, 2, 3]
stderr:   role_ranks=[0, 1, 2, 3]
stderr:   global_ranks=[0, 1, 2, 3]
stderr:   role_world_sizes=[4, 4, 4, 4]
stderr:   global_world_sizes=[4, 4, 4, 4]
stderr: 
stderr: INFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group
stderr: INFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /tmp/torchelastic_hqci49x8/none_273htobb/attempt_0/0/error.json
stderr: INFO:torch.distributed.elastic.multiprocessing:Setting worker1 reply file to: /tmp/torchelastic_hqci49x8/none_273htobb/attempt_0/1/error.json
stderr: INFO:torch.distributed.elastic.multiprocessing:Setting worker2 reply file to: /tmp/torchelastic_hqci49x8/none_273htobb/attempt_0/2/error.json
stderr: INFO:torch.distributed.elastic.multiprocessing:Setting worker3 reply file to: /tmp/torchelastic_hqci49x8/none_273htobb/attempt_0/3/error.json
stderr: INFO:torch.distributed.elastic.agent.server.api:[default] worker group successfully finished. Waiting 300 seconds for other agents to finish.
stderr: INFO:torch.distributed.elastic.agent.server.api:Local worker group finished (SUCCEEDED). Waiting 300 seconds for other agents to finish
stderr: /gpfs/home/jsolt/FourierNN/torchenv/lib/python3.7/site-packages/torch/distributed/elastic/utils/store.py:71: FutureWarning: This is an experimental API and will be changed in future.
stderr:   "This is an experimental API and will be changed in future.", FutureWarning
stderr: INFO:torch.distributed.elastic.agent.server.api:Done waiting for other agents. Elapsed: 0.0003952980041503906 seconds
stderr: {"name": "torchelastic.worker.status.SUCCEEDED", "source": "WORKER", "timestamp": 0, "metadata": {"run_id": "none", "global_rank": 0, "group_rank": 0, "worker_id": "9917", "role": "default", "hostname": "gpu2004.oscar.ccv.brown.edu", "state": "SUCCEEDED", "total_run_time": 25, "rdzv_backend": "static", "raw_error": null, "metadata": "{\"group_world_size\": 1, \"entry_point\": \"python3\", \"local_rank\": [0], \"role_rank\": [0], \"role_world_size\": [4]}", "agent_restarts": 0}}
stderr: {"name": "torchelastic.worker.status.SUCCEEDED", "source": "WORKER", "timestamp": 0, "metadata": {"run_id": "none", "global_rank": 1, "group_rank": 0, "worker_id": "9918", "role": "default", "hostname": "gpu2004.oscar.ccv.brown.edu", "state": "SUCCEEDED", "total_run_time": 25, "rdzv_backend": "static", "raw_error": null, "metadata": "{\"group_world_size\": 1, \"entry_point\": \"python3\", \"local_rank\": [1], \"role_rank\": [1], \"role_world_size\": [4]}", "agent_restarts": 0}}
stderr: {"name": "torchelastic.worker.status.SUCCEEDED", "source": "WORKER", "timestamp": 0, "metadata": {"run_id": "none", "global_rank": 2, "group_rank": 0, "worker_id": "9919", "role": "default", "hostname": "gpu2004.oscar.ccv.brown.edu", "state": "SUCCEEDED", "total_run_time": 25, "rdzv_backend": "static", "raw_error": null, "metadata": "{\"group_world_size\": 1, \"entry_point\": \"python3\", \"local_rank\": [2], \"role_rank\": [2], \"role_world_size\": [4]}", "agent_restarts": 0}}
stderr: {"name": "torchelastic.worker.status.SUCCEEDED", "source": "WORKER", "timestamp": 0, "metadata": {"run_id": "none", "global_rank": 3, "group_rank": 0, "worker_id": "9922", "role": "default", "hostname": "gpu2004.oscar.ccv.brown.edu", "state": "SUCCEEDED", "total_run_time": 25, "rdzv_backend": "static", "raw_error": null, "metadata": "{\"group_world_size\": 1, \"entry_point\": \"python3\", \"local_rank\": [3], \"role_rank\": [3], \"role_world_size\": [4]}", "agent_restarts": 0}}
stderr: {"name": "torchelastic.worker.status.SUCCEEDED", "source": "AGENT", "timestamp": 0, "metadata": {"run_id": "none", "global_rank": null, "group_rank": 0, "worker_id": null, "role": "default", "hostname": "gpu2004.oscar.ccv.brown.edu", "state": "SUCCEEDED", "total_run_time": 25, "rdzv_backend": "static", "raw_error": null, "metadata": "{\"group_world_size\": 1, \"entry_point\": \"python3\"}", "agent_restarts": 0}}

Running:  accelerate-launch --config_file=/users/jsolt/.cache/huggingface/accelerate/default_config.yaml /gpfs/home/jsolt/FourierNN/torchenv/lib/python3.7/site-packages/accelerate/test_utils/test_script.py
stdout: **Initialization**
stdout: Testing, testing. 1, 2, 3.
stdout: Distributed environment: MULTI_GPU
stdout: Num processes: 4
stdout: Process index: 0
stdout: Local process index: 0
stdout: Device: cuda:0
stdout: Use FP16 precision: False
stdout: 
stdout: 
stdout: **Test random number generator synchronization**
stdout: Distributed environment: MULTI_GPU
stdout: Num processes: 4
stdout: Process index: 3
stdout: Local process index: 3
stdout: Device: cuda:3
stdout: Use FP16 precision: False
stdout: 
stdout: Distributed environment: MULTI_GPU
stdout: Num processes: 4
stdout: Process index: 1
stdout: Local process index: 1
stdout: Device: cuda:1
stdout: Use FP16 precision: False
stdout: 
stdout: Distributed environment: MULTI_GPU
stdout: Num processes: 4
stdout: Process index: 2
stdout: Local process index: 2
stdout: Device: cuda:2
stdout: Use FP16 precision: False
stdout: 
stdout: All rng are properly synched.
stdout: 
stdout: **DataLoader integration test**
stdout: Non-shuffled dataloader passing.
stdout: Shuffled dataloader passing.
stdout: 
stdout: **Training integration test**
stdout: Training yielded the same results on one CPU or distributed setup with no batch split.
stdout: Training yielded the same results on one CPU or distributes setup with batch split.
stdout: *****************************************
stdout: Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed.
stdout: *****************************************
Test is a success! You are ready for your distributed training!
Model: multithread_test_4_gpus
Loading data from ../data/shared/LaPlanteSims/v10/t21_snapshots_nowedge.hdf5...
Data Tensor shape: [160, 30, 512, 512]
Labels Tensor shape: [160, 2]
Loading data from ../data/shared/LaPlanteSims/v10/t21_snapshots_nowedge.hdf5...
Data Tensor shape: [40, 30, 512, 512]
Labels Tensor shape: [40, 2]
Using cuda device
Epoch 1
-------------------------------
/gpfs/home/jsolt/FourierNN/torchenv/lib/python3.7/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)
  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
Average loss: 11.562568283081054
0.05
Epoch 2
-------------------------------
Average loss: 10.217938232421876
0.03333333333333333
Epoch 3
-------------------------------
Average loss: 1.3582291543483733
0.025
Epoch 4
-------------------------------
Average loss: 0.17868733182549476
0.020000000000000004
Epoch 5
-------------------------------
Average loss: 0.05191709790378809
0.016666666666666666
Epoch 6
-------------------------------
Average loss: 0.05599275194108486
0.014285714285714285
Epoch 7
-------------------------------
Average loss: 0.03632122976705432
0.0125
Epoch 8
-------------------------------
Average loss: 0.03606199435889721
0.011111111111111112
Epoch 9
-------------------------------
Average loss: 0.04326795488595962
0.010000000000000002
Epoch 10
-------------------------------
Average loss: 0.032316235918551685
0.009090909090909092
Saving PyTorch Model State to models/multithread_test_4_gpus/multithread_test_4_gpus.pth.pth...
Model Saved.
Saving loss data to models/multithread_test_4_gpus/loss_multithread_test_4_gpus.npz...
Loss data saved.
Generating hyperparameter summary at models/multithread_test_4_gpus/hp_multithread_test_4_gpus.json...
Hyperparameter summary saved.

* * * * * * * *
PROCESS TIME: 0:00:53.815885
* * * * * * * *
Saving loss plot to models/multithread_test_4_gpus/multithread_test_4_gpus_loss.png...
Loss plot saved.

Loading data from ../data/shared/LaPlanteSims/v10/t21_snapshots_nowedge.hdf5...
Data Tensor shape: [40, 30, 512, 512]
Labels Tensor shape: [40, 2]
Using cuda device
Loading model state from models/multithread_test_4_gpus/multithread_test_4_gpus.pth
Model loaded.
Predicting on 40 samples...
/gpfs/home/jsolt/FourierNN/torchenv/lib/python3.7/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)
  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
Saving prediction to models/multithread_test_4_gpus/pred_multithread_test_4_gpus.npz...
(40, 2)
Prediction saved.
Plotting results for parameter: midpoint...
Plotting results for parameter: duration...
